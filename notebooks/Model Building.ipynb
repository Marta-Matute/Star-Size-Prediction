{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eee298b3",
   "metadata": {},
   "source": [
    "# 2. Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9559f47a",
   "metadata": {},
   "source": [
    "We have a dataset ready to be worked with. Using the pickle we created, we will now try with several different predicting models to see which ones gives us the best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8548528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from plotly.subplots import make_subplots\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import *\n",
    "import warnings\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "\n",
    "\n",
    "#models\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# ensemble models\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "if not os.path.isdir('data'):\n",
    "    os.mkdir('data')\n",
    "if not os.path.isdir('models'):\n",
    "    os.mkdir('models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e182d6",
   "metadata": {},
   "source": [
    "## 2.1. Basic model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd24dcc1",
   "metadata": {},
   "source": [
    "We will now start modeling our data. Our first step will be to choose among the most common classifiers and perform a basic model with our data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45328b9e",
   "metadata": {},
   "source": [
    "Let's load the previously pre-processed data from the pickle in order to work on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9d4930cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/train_test.pickle', 'rb') as file:\n",
    "    dataset_list = pickle.load(file)\n",
    "    \n",
    "X_train = dataset_list[0]\n",
    "X_test = dataset_list[1]\n",
    "y_train = dataset_list[2]\n",
    "y_test = dataset_list[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22b5bdc",
   "metadata": {},
   "source": [
    "The models we will be trying are:\n",
    "- Logistic Regression\n",
    "- K-Neighbors Classifier\n",
    "- Decision Tree Classifier\n",
    "- XGBoost\n",
    "- Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2a15edb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [01:00<00:00, 12.12s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.959365</td>\n",
       "      <td>0.003124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.956494</td>\n",
       "      <td>0.002893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.951391</td>\n",
       "      <td>0.003934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K-Neighbors Classifier</td>\n",
       "      <td>0.941496</td>\n",
       "      <td>0.004220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Decision Tree Classifier</td>\n",
       "      <td>0.865726</td>\n",
       "      <td>0.007224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Model      Mean       Std\n",
       "3                   XGBoost  0.959365  0.003124\n",
       "4             Random Forest  0.956494  0.002893\n",
       "0       Logistic Regression  0.951391  0.003934\n",
       "1    K-Neighbors Classifier  0.941496  0.004220\n",
       "2  Decision Tree Classifier  0.865726  0.007224"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_folds = 10\n",
    "seed = 1\n",
    "scoring = 'roc_auc'\n",
    "model_names = ['Logistic Regression', 'K-Neighbors Classifier', 'Decision Tree Classifier', 'XGBoost', 'Random Forest' ]\n",
    "models = []\n",
    "# basic models\n",
    "models.append((model_names[0], LogisticRegression()))\n",
    "models.append((model_names[1], KNeighborsClassifier()))\n",
    "models.append((model_names[2], DecisionTreeClassifier()))\n",
    "\n",
    "# ensemble models\n",
    "models.append((model_names[3], XGBClassifier(use_label_encoder=False,eval_metric='auc')))\n",
    "models.append((model_names[4], RandomForestClassifier()))\n",
    "\n",
    "# KFolds for model selection:\n",
    "table_results = []\n",
    "basic_results = {}\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    for name, model in tqdm(models):\n",
    "        kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "        cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n",
    "        table_results.append([name,cv_results.mean(),cv_results.std()])\n",
    "        basic_results[name] = cv_results.mean()\n",
    "\n",
    "pd.DataFrame(table_results, columns=['Model', 'Mean', 'Std']).sort_values(by = ['Mean'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c760a",
   "metadata": {},
   "source": [
    "We see that we get really good results for most of the models, except for the decision tree classifier, that only gives around 86% accuracy (the next best one is already at 94% accuracy). For this reason we will not be further using the Decision Tree Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "268d8f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the Decision tree classifier from our list of models\n",
    "if(models[2][0] == 'Decision Tree Classifier'):\n",
    "    del(models[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd923657",
   "metadata": {},
   "source": [
    "## 2.2. Feature importance for the best models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52095eb1",
   "metadata": {},
   "source": [
    "#### Feature importance for XGBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "47b176ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "xgb_clf = models[2][1].fit(X_train, y_train)\n",
    "xgb.plot_importance(booster=xgb_clf, title='Feature Importance for XGBoost Classifier');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc3c28d",
   "metadata": {},
   "source": [
    "#### Feature importance for Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a2b850c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "rf_clf = models[3][1].fit(X_train, y_train)\n",
    "\n",
    "x_axis = X_train.columns\n",
    "y_axis = rf_clf.feature_importances_\n",
    "plt.barh(x_axis, y_axis, height=0.15)\n",
    "plt.title('Feature Importance for Random Forest Classifier')\n",
    "plt.ylabel('Feature')\n",
    "plt.xlabel('F_score')\n",
    "plt.grid()\n",
    "for i, v in enumerate(y_axis):\n",
    "    plt.text(v+0.01, i-0.05, str(round(v, 2)), color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64afefa7",
   "metadata": {},
   "source": [
    "#### Feature importance for Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a6bb818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_clf = models[0][1].fit(X_train, y_train)\n",
    "\n",
    "x_axis = X_train.columns\n",
    "y_axis = abs(lr_clf.coef_[0])\n",
    "plt.barh(x_axis, y_axis, height=0.15)\n",
    "plt.title('Feature Importance for Logistic Regression Classifier')\n",
    "plt.ylabel('Feature')\n",
    "plt.xlabel('F_score')\n",
    "plt.grid()\n",
    "for i, v in enumerate(y_axis):\n",
    "    plt.text(v+0.01, i-0.05, str(round(v, 2)), color='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae5a695",
   "metadata": {},
   "source": [
    "From the three plots, we can certainly conclude that the star's color plays the biggest role when it comes to classifiying the stars, as well as the absolute magnitude, as we could have expected. The distance from Earth and the apparent visual magnitude seem to not be so important. That would make sense, since how far from us a star is has nothing to do with how big or small it is. Similarly, the apparent magnitude we see form Earth doesn't determine its size. \n",
    "\n",
    "The next question is: would our models classify better if we removed the three variables that are not so improtant? Let's try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0e22e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:43<00:00, 10.85s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Mean</th>\n",
       "      <th>Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.955922</td>\n",
       "      <td>0.003188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.951288</td>\n",
       "      <td>0.004040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.948011</td>\n",
       "      <td>0.003807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>K-Neighbors Classifier</td>\n",
       "      <td>0.938777</td>\n",
       "      <td>0.003282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Model      Mean       Std\n",
       "2                 XGBoost  0.955922  0.003188\n",
       "0     Logistic Regression  0.951288  0.004040\n",
       "3           Random Forest  0.948011  0.003807\n",
       "1  K-Neighbors Classifier  0.938777  0.003282"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_fi = X_train.drop(['Vmag', 'Plx', 'e_Plx'], axis=1)\n",
    "\n",
    "# KFolds for model selection:\n",
    "table_results = []\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    for name, model in tqdm(models):\n",
    "        kfold = KFold(n_splits=num_folds, random_state=seed, shuffle=True)\n",
    "        cv_results = cross_val_score(model, X_train_fi, y_train, cv=kfold, scoring=scoring)\n",
    "        table_results.append([name,cv_results.mean(),cv_results.std()])\n",
    "\n",
    "pd.DataFrame(table_results, columns=['Model', 'Mean', 'Std']).sort_values(by = ['Mean'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a731c8",
   "metadata": {},
   "source": [
    "This change made the mean for all the classifying values worse than they were before. Thus, we will keep all the variables just as we had before. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f58874e",
   "metadata": {},
   "source": [
    "## 2.3. Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca091be",
   "metadata": {},
   "source": [
    "We will now tune the best three models we found,i.e. XGBoost, Logistic Regression and Random Forest. To do so, we will create a grid with a few possible values to pass as arguments to the model. Then, we'll try the results with each combination of parameters, and by comparison, choose the one that yields the best results. For that we will use a grid search (trying all the combinations from the grid we created). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1750db03",
   "metadata": {},
   "source": [
    "### 2.3.1. XGBoost Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dcc9ff25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "After the hyperparameter tuning, the best score achieved has been 0.9616300617164351.\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'max_depth': [max_depth for max_depth in range(5,10)],\n",
    "               'min_child_weight': [min_child_weight for min_child_weight in range(5,10)],\n",
    "               'eta': [.5, .4, .3, .2, .1, .05, .01, .005],\n",
    "              }\n",
    "\n",
    "xgbc = GridSearchCV(estimator = XGBClassifier(use_label_encoder=False,eval_metric='auc', objective='reg:squarederror'), param_grid = param_grid, cv = 5, verbose=2, n_jobs=-1, scoring=scoring)\n",
    "\n",
    "hypertuned_xgb = xgbc.fit(X_train, y_train)\n",
    "print(f\"After the hyperparameter tuning, the best score achieved has been {hypertuned_xgb.best_score_}.\")\n",
    "hypertuning_results['XGBoost']= hypertuned_xgb.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df58d1",
   "metadata": {},
   "source": [
    "We have accomplished a better result than the original one we had with the new parameters. We will save this model so that we can have acces to it later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9ed08ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir('models'):\n",
    "    os.mkdir('models')\n",
    "\n",
    "with open('models/hypertuned_xgbc.pickle', 'wb') as file:\n",
    "    pickle.dump(hypertuned_xgb, file)dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c7b64d",
   "metadata": {},
   "source": [
    "### 2.3.2. Random Forest hyperparameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0cbb2d2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n",
      "After the hyperparameter tuning, the best score achieved has been 0.9589651535569805.\n"
     ]
    }
   ],
   "source": [
    "# Create the random grid\n",
    "random_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 5)],\n",
    "               'max_features': ['auto', 'sqrt'],\n",
    "               'min_samples_split': [2, 7],\n",
    "               'min_samples_leaf': [1, 3],\n",
    "               'bootstrap': [True, False]\n",
    "              }\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestClassifier()\n",
    "# Random search of parameters, using 5 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = GridSearchCV(estimator = rf, param_grid = random_grid, cv = 5, verbose=2, n_jobs=-1, scoring=scoring)\n",
    "# Fit the random search model\n",
    "hypertuned_rf = rf_random.fit(X_train, y_train)\n",
    "\n",
    "print(f\"After the hyperparameter tuning, the best score achieved has been {hypertuned_rf.best_score_}.\")\n",
    "hypertuning_results['Random Forest']= hypertuned_rf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5e2d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/hypertuned_rf.pickle', 'wb') as file:\n",
    "    pickle.dump(hypertuned_rf, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d8d8a6",
   "metadata": {},
   "source": [
    "### 2.3.3. Logistic Regression Hypertuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "062dbcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 320 candidates, totalling 1600 fits\n",
      "After the hyperparameter tuning, the best score achieved has been 0.9516406426340046.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "# Create first pipeline for base without reducing features.\n",
    "pipe = Pipeline([('classifier' , LogisticRegression())])\n",
    "\n",
    "# Create param grid.\n",
    "param_grid = [\n",
    "    {'classifier__C' : np.logspace(-4, 4, 20),\n",
    "     'classifier__fit_intercept' : [True, False],\n",
    "     'classifier__class_weight' : [dict, 'balanced'],\n",
    "     'classifier__solver' : ['newton-cg', 'lbfgs', 'sag', 'saga']}\n",
    "]\n",
    "\n",
    "# Create grid search object\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    clf = GridSearchCV(pipe, param_grid = param_grid, cv = 5, verbose=2, n_jobs=-1, scoring=scoring)\n",
    "\n",
    "# Fit on data\n",
    "hypertuned_logreg = clf.fit(X_train, y_train)\n",
    "\n",
    "print(f\"After the hyperparameter tuning, the best score achieved has been {hypertuned_logreg.best_score_}.\")\n",
    "hypertuning_results['Logistic Regression']= hypertuned_logreg.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0f1971",
   "metadata": {},
   "source": [
    "We have now successfully found the best parameters for the Logistic regression model as well. We'll save it, as we did with with the XGBosst classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ceebf5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/hypertuned_logreg.pickle', 'wb') as file:\n",
    "    pickle.dump(hypertuned_logreg, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e9be86",
   "metadata": {},
   "source": [
    "## 2.4. Ensemble Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871e037f",
   "metadata": {},
   "source": [
    "Let's have an overview of what we have done so far. We have considered 5 differents models, of which we discarded one for not having results as good as the others. We have then tuned the parameters of the best three of the remaining models as to get the highest Area Under the Curve possible. Now, as our final step before analysing the results, we will make an ensemble model. That means we will take the three hypertuned models, and we will combine them in one single `Voting Classifier Model`. This will take the results of each model and through an *election* decide which class to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "52fa2e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names.append('Ensemble')\n",
    "estimators = [('xgb', hypertuned_xgb), ('rf', hypertuned_rf), ('lr', hypertuned_logreg)]#, ('knn', hypertuned_knn)]\n",
    "voting_clf_ = VotingClassifier(estimators=estimators, voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ed699d3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marta\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.94965936 0.94965936        nan 0.94965923 0.94965786 0.94962292\n",
      " 0.94962289        nan 0.94962332 0.9496231  0.94972737 0.94972694\n",
      " 0.94971966 0.94972728 0.94972749 0.94971559 0.94971553 0.9497155\n",
      " 0.94971633 0.94971553 0.95028347 0.9502835         nan 0.95028329\n",
      " 0.95028326 0.9501416  0.95014157        nan 0.95014157 0.9501413\n",
      " 0.95037753 0.95037747 0.95036064 0.95037737 0.95037734 0.95034152\n",
      " 0.95034155 0.95034164 0.95034167 0.95034142 0.95041035 0.95041035\n",
      "        nan 0.95041014 0.95041056 0.95013584 0.95013584        nan\n",
      " 0.95013618 0.95013615 0.95054173 0.95054145 0.95052371 0.95054136\n",
      " 0.95054054 0.95048189 0.95048182 0.95048201 0.95048097 0.9504814\n",
      " 0.95043613 0.95043604        nan 0.95043705 0.95043616 0.95009802\n",
      " 0.95009802        nan 0.95009826 0.95009759 0.95058012 0.95058012\n",
      " 0.95057528 0.95058064 0.9505796  0.95055387 0.95055381 0.95055384\n",
      " 0.95055365 0.95055368 0.95046669 0.9504666         nan 0.9504666\n",
      " 0.95046578 0.95015719 0.95015722        nan 0.95015621 0.95015593\n",
      " 0.9506078  0.9506078  0.95061334 0.95060728 0.95060799 0.9506522\n",
      " 0.95065229 0.95065248 0.9506522  0.95065183 0.95051427 0.95051439\n",
      "        nan 0.95051412 0.95051341 0.95024381 0.95024365        nan\n",
      " 0.95024329 0.95024332 0.95064635 0.95064657 0.95065226 0.95064626\n",
      " 0.95064688 0.95074319 0.95074338 0.9507435  0.95074286 0.95074154\n",
      " 0.95054664 0.95054664        nan 0.95054695 0.95054692 0.95029943\n",
      " 0.9502994         nan 0.95029922 0.95029882 0.95067466 0.9506746\n",
      " 0.95067643 0.95067362 0.95067404 0.95079664 0.95079664 0.95079658\n",
      " 0.95079609 0.95079701 0.95056344 0.95056341        nan 0.9505639\n",
      " 0.95056405 0.95032504 0.95032504        nan 0.95032525 0.95032467\n",
      " 0.95068696 0.9506869  0.95068784 0.9506869  0.95068601 0.95081947\n",
      " 0.95081944 0.9508191  0.95081812 0.95081834 0.95056812 0.95056809\n",
      "        nan 0.9505691  0.95057008 0.95033967 0.95033967        nan\n",
      " 0.95033954 0.9503385  0.95068922 0.95068922 0.95068965 0.95068876\n",
      " 0.95068855 0.95082611 0.9508262  0.95082599 0.95082289 0.95082409\n",
      " 0.95056968 0.95056956        nan 0.95056999 0.95057155 0.95034373\n",
      " 0.95034373        nan 0.95034398 0.95034328 0.9506895  0.95068934\n",
      " 0.95068953 0.95069008 0.95068959 0.95082782 0.95082776 0.95082788\n",
      " 0.95082608 0.95082791 0.95057014 0.95057011        nan 0.95057072\n",
      " 0.9505724  0.95034624 0.950346          nan 0.95034627 0.95034542\n",
      " 0.95069188 0.95069173 0.95069164 0.95069063 0.95069115 0.95082908\n",
      " 0.95082908 0.95082904 0.9508273  0.95082911 0.9505706  0.95057069\n",
      "        nan 0.95057115 0.95057262 0.95034698 0.95034683        nan\n",
      " 0.95034683 0.95034649 0.95069219 0.95069207 0.95069231 0.95069124\n",
      " 0.95069124 0.95082914 0.95082911 0.9508292  0.95082742 0.95082981\n",
      " 0.95057072 0.95057078        nan 0.95057151 0.95057243 0.95034731\n",
      " 0.95034722        nan 0.95034698 0.95034692 0.95069222 0.95069234\n",
      " 0.95069231 0.95069155 0.9506917  0.95082908 0.95082901 0.95082926\n",
      " 0.95082736 0.95082816 0.95057063 0.95057075        nan 0.95057136\n",
      " 0.95057252 0.95034728 0.95034719        nan 0.95034701 0.95034643\n",
      " 0.95069237 0.95069231 0.95069259 0.9506913  0.95069133 0.95082908\n",
      " 0.95082898 0.95082926 0.95082758 0.95082852 0.95057057 0.95057081\n",
      "        nan 0.95057142 0.95057292 0.95034735 0.95034731        nan\n",
      " 0.95034704 0.95034664 0.95069249 0.95069234 0.95069259 0.95069145\n",
      " 0.95069197 0.95082917 0.95082911 0.95082917 0.95082742 0.95082981\n",
      " 0.95057057 0.95057078        nan 0.95057142 0.95057259 0.95034735\n",
      " 0.95034738        nan 0.95034738 0.95034689 0.95069246 0.9506924\n",
      " 0.95069262 0.95069148 0.95069161 0.95082917 0.95082911 0.95082914\n",
      " 0.95082733 0.95082972 0.9505706  0.95057078        nan 0.95057145\n",
      " 0.95057295 0.95034738 0.95034741        nan 0.95034731 0.95034692\n",
      " 0.95069246 0.95069243 0.95069262 0.95069158 0.95069127 0.95082923\n",
      " 0.95082917 0.95082914 0.95082788 0.95082874 0.9505706  0.95057078\n",
      "        nan 0.95057136 0.95057243 0.95034741 0.95034744        nan\n",
      " 0.95034722 0.95034643 0.95069246 0.95069243 0.95069262 0.95069155\n",
      " 0.95069191 0.95082923 0.95082914 0.95082914 0.95082666 0.95082996\n",
      " 0.9505706  0.95057081        nan 0.95057133 0.95057301 0.95034738\n",
      " 0.95034747        nan 0.95034698 0.95034658 0.95069246 0.95069246\n",
      " 0.95069262 0.95069133 0.95069164 0.95082923 0.95082914 0.95082914\n",
      " 0.95082816 0.95082947 0.9505706  0.95057081        nan 0.95057145\n",
      " 0.95057262 0.95034741 0.95034747        nan 0.95034725 0.95034695\n",
      " 0.95069243 0.95069246 0.95069262 0.95069155 0.95069173 0.95082923\n",
      " 0.95082914 0.95082914 0.95082761 0.95082926]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marta\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.95167784 0.95167784        nan 0.95167692 0.95167735 0.95166973\n",
      " 0.95166943        nan 0.95167041 0.95166961 0.95173337 0.95173208\n",
      " 0.95173273 0.95173263 0.95173318 0.95173193 0.9517319  0.95173193\n",
      " 0.95172979 0.95173129 0.95231642 0.95231648        nan 0.95231593\n",
      " 0.95231572 0.95219371 0.95219362        nan 0.95219497 0.95219423\n",
      " 0.95240624 0.95240637 0.95238669 0.95240621 0.95240569 0.95237012\n",
      " 0.95237012 0.95237055 0.95237    0.95237043 0.95245459 0.95245459\n",
      "        nan 0.95245358 0.95245422 0.95219731 0.95219731        nan\n",
      " 0.95219734 0.95219798 0.95257363 0.9525735  0.95255291 0.95257378\n",
      " 0.95257375 0.95250832 0.95250825 0.95250804 0.95250911 0.95250819\n",
      " 0.95246659 0.95246656        nan 0.95246662 0.95246683 0.95214831\n",
      " 0.95214825        nan 0.95214801 0.95214761 0.95258673 0.95258673\n",
      " 0.95258062 0.95258667 0.95258682 0.95255718 0.95255721 0.95255724\n",
      " 0.95255714 0.95255727 0.95249379 0.95249391        nan 0.95249354\n",
      " 0.95249385 0.95220161 0.95220164        nan 0.95220109 0.95220109\n",
      " 0.95262276 0.95262276 0.95262646 0.95262233 0.95262175 0.95265696\n",
      " 0.95265693 0.95265699 0.95265702 0.95265657 0.95253551 0.95253584\n",
      "        nan 0.95253563 0.95253486 0.95228324 0.95228333        nan\n",
      " 0.95228291 0.95228245 0.95264813 0.95264804 0.95265339 0.95264813\n",
      " 0.95264697 0.95273939 0.95273942 0.95273915 0.95273918 0.95273964\n",
      " 0.95255528 0.95255537        nan 0.9525547  0.95255672 0.95233462\n",
      " 0.95233462        nan 0.95233489 0.95233444 0.95266583 0.9526658\n",
      " 0.95266971 0.95266573 0.95266506 0.95278503 0.95278506 0.95278509\n",
      " 0.95278441 0.95278512 0.9525536  0.95255397        nan 0.95255266\n",
      " 0.95255623 0.95235782 0.95235773        nan 0.95235733 0.95235782\n",
      " 0.95267154 0.95267148 0.95267212 0.95267035 0.95267056 0.9528026\n",
      " 0.95280251 0.95280282 0.95280266 0.95280532 0.95255079 0.95255088\n",
      "        nan 0.95255088 0.95255373 0.95236424 0.95236427        nan\n",
      " 0.95236365 0.95236271 0.95267087 0.9526709  0.95267142 0.95267093\n",
      " 0.95266922 0.95280523 0.95280526 0.95280526 0.95280606 0.9528067\n",
      " 0.95254945 0.95254945        nan 0.95255027 0.95255247 0.95236861\n",
      " 0.95236845        nan 0.95236818 0.95236677 0.9526665  0.95266644\n",
      " 0.9526665  0.95266723 0.95267011 0.95280664 0.9528067  0.95280667\n",
      " 0.95280694 0.95280795 0.95255009 0.95255006        nan 0.95255156\n",
      " 0.95255406 0.95237035 0.95237047        nan 0.95236913 0.95236977\n",
      " 0.95266512 0.95266491 0.95266473 0.95266503 0.95266864 0.9528063\n",
      " 0.95280627 0.9528063  0.95280737 0.95280807 0.95254979 0.95254957\n",
      "        nan 0.95255046 0.95255312 0.95237062 0.95237059        nan\n",
      " 0.95237068 0.95236977 0.95266326 0.9526632  0.95266299 0.95266412\n",
      " 0.95266717 0.95280612 0.95280612 0.95280612 0.95280704 0.95280896\n",
      " 0.95254948 0.95254966        nan 0.95254939 0.95255327 0.95237133\n",
      " 0.95237145        nan 0.95237059 0.95236955 0.9526624  0.95266234\n",
      " 0.95266305 0.95266418 0.95266675 0.95280603 0.95280594 0.95280606\n",
      " 0.95280658 0.95280911 0.9525492  0.95254933        nan 0.95254927\n",
      " 0.95255293 0.95237148 0.95237154        nan 0.95237075 0.95237001\n",
      " 0.95266204 0.95266195 0.95266292 0.95266387 0.95266696 0.95280652\n",
      " 0.95280633 0.95280646 0.95280691 0.95280905 0.95254896 0.95254914\n",
      "        nan 0.95254945 0.95255305 0.95237169 0.95237172        nan\n",
      " 0.95237078 0.95237017 0.95266195 0.95266188 0.95266289 0.95266369\n",
      " 0.95266668 0.95280639 0.95280633 0.95280643 0.95280688 0.95280942\n",
      " 0.95254887 0.95254908        nan 0.95255003 0.95255333 0.95237166\n",
      " 0.95237169        nan 0.95237072 0.95237017 0.95266188 0.95266185\n",
      " 0.95266289 0.95266354 0.95266772 0.95280643 0.95280636 0.95280643\n",
      " 0.95280694 0.95280924 0.95254881 0.95254905        nan 0.95254936\n",
      " 0.95255293 0.95237172 0.95237175        nan 0.95237068 0.95236989\n",
      " 0.95266188 0.95266182 0.95266289 0.95266354 0.95266616 0.95280639\n",
      " 0.9528063  0.95280639 0.95280691 0.95280933 0.95254881 0.95254902\n",
      "        nan 0.95254972 0.95255342 0.95237175 0.95237175        nan\n",
      " 0.95237068 0.95236983 0.95266185 0.95266182 0.95266283 0.95266369\n",
      " 0.95266619 0.95280639 0.9528063  0.95280639 0.95280691 0.95280933\n",
      " 0.95254881 0.95254905        nan 0.95255    0.95255247 0.95237175\n",
      " 0.95237178        nan 0.95237075 0.95236992 0.95266185 0.95266182\n",
      " 0.95266283 0.95266372 0.95266726 0.95280643 0.9528063  0.95280639\n",
      " 0.95280685 0.95280905 0.95254881 0.95254905        nan 0.95254997\n",
      " 0.95255376 0.95237175 0.95237178        nan 0.95237065 0.95236974\n",
      " 0.95266185 0.95266182 0.95266283 0.95266375 0.95266604 0.95280643\n",
      " 0.9528063  0.95280639 0.95280673 0.95280939]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marta\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.95041237 0.95041234        nan 0.95041212 0.95041218 0.95038965\n",
      " 0.95038974        nan 0.9503899  0.9503895  0.9504723  0.95047236\n",
      " 0.95046769 0.95047297 0.95047303 0.95046378 0.95046366 0.95046341\n",
      " 0.9504632  0.95046265 0.95110175 0.95110193        nan 0.95110111\n",
      " 0.95110096 0.95096077 0.9509608         nan 0.95096061 0.95095991\n",
      " 0.95119071 0.95119068 0.95117209 0.95119099 0.95119068 0.95115365\n",
      " 0.95115362 0.9511539  0.95115393 0.95115271 0.9512757  0.9512757\n",
      "        nan 0.9512753  0.95127551 0.95101235 0.95101235        nan\n",
      " 0.95101208 0.95101263 0.9513947  0.95139479 0.95137361 0.95139467\n",
      " 0.9513951  0.95132732 0.95132742 0.95132778 0.95132626 0.95132745\n",
      " 0.95130905 0.95130908        nan 0.9513095  0.95130892 0.95099417\n",
      " 0.95099411        nan 0.95099441 0.95099362 0.95143809 0.95143816\n",
      " 0.95143205 0.95143812 0.95143773 0.95140573 0.9514058  0.9514061\n",
      " 0.95140534 0.95140546 0.95136379 0.95136355        nan 0.95136364\n",
      " 0.9513637  0.95106052 0.95106042        nan 0.95105963 0.95105865\n",
      " 0.95148939 0.95148933 0.9514925  0.95148936 0.95148966 0.95151787\n",
      " 0.95151809 0.951518   0.95151769 0.95151833 0.95141007 0.95140992\n",
      "        nan 0.95140928 0.95140833 0.95114032 0.95114032        nan\n",
      " 0.9511402  0.95113956 0.95151797 0.95151788 0.95152172 0.95151739\n",
      " 0.95151696 0.95159762 0.95159765 0.95159792 0.95159621 0.95159527\n",
      " 0.95143447 0.95143435        nan 0.95143477 0.95143609 0.95119757\n",
      " 0.95119766        nan 0.95119827 0.95119857 0.95153937 0.95153943\n",
      " 0.95154074 0.95153839 0.95153763 0.9516476  0.95164747 0.95164769\n",
      " 0.95164741 0.95164631 0.95144329 0.95144311        nan 0.95144229\n",
      " 0.95144302 0.95122349 0.95122333        nan 0.95122434 0.95122388\n",
      " 0.95154694 0.95154691 0.95154835 0.9515467  0.9515496  0.95166625\n",
      " 0.95166625 0.95166619 0.95166643 0.95166729 0.95144558 0.95144552\n",
      "        nan 0.95144586 0.95144726 0.95123228 0.95123204        nan\n",
      " 0.951231   0.95123139 0.95154676 0.95154682 0.95154734 0.9515474\n",
      " 0.95154957 0.9516733  0.9516733  0.95167336 0.95167349 0.95167446\n",
      " 0.9514472  0.95144711        nan 0.95144696 0.95144888 0.95123405\n",
      " 0.95123405        nan 0.9512353  0.95123475 0.95154649 0.95154652\n",
      " 0.95154716 0.95154688 0.95154902 0.9516755  0.95167535 0.95167553\n",
      " 0.95167663 0.95167623 0.951448   0.95144827        nan 0.95144738\n",
      " 0.95144958 0.95123466 0.95123469        nan 0.95123442 0.95123674\n",
      " 0.95154673 0.95154667 0.95154719 0.95154646 0.95154826 0.95167703\n",
      " 0.95167663 0.95167749 0.95167788 0.9516784  0.95144809 0.95144836\n",
      "        nan 0.95144821 0.95144873 0.95123628 0.95123631        nan\n",
      " 0.95123481 0.95123692 0.95154707 0.95154704 0.95154728 0.95154585\n",
      " 0.95154783 0.95167752 0.95167712 0.95167739 0.95167846 0.95167855\n",
      " 0.95144845 0.95144848        nan 0.95144873 0.95144952 0.95123655\n",
      " 0.95123643        nan 0.95123521 0.95123716 0.95154698 0.95154704\n",
      " 0.95154725 0.95154661 0.95154768 0.95167715 0.95167715 0.95167724\n",
      " 0.95167874 0.95167892 0.95144839 0.95144845        nan 0.95144861\n",
      " 0.95144958 0.9512368  0.95123683        nan 0.9512353  0.95123631\n",
      " 0.95154701 0.95154694 0.9515471  0.95154658 0.95154798 0.95167721\n",
      " 0.95167721 0.9516773  0.95167904 0.95167868 0.95144824 0.95144827\n",
      "        nan 0.95144854 0.95145007 0.95123677 0.95123671        nan\n",
      " 0.95123533 0.95123677 0.95154704 0.95154704 0.95154694 0.95154643\n",
      " 0.95154899 0.95167721 0.95167724 0.95167742 0.9516791  0.95167855\n",
      " 0.95144836 0.95144821        nan 0.95144858 0.95145022 0.95123665\n",
      " 0.95123665        nan 0.95123536 0.95123655 0.9515471  0.95154707\n",
      " 0.95154691 0.95154643 0.95154801 0.95167721 0.95167715 0.95167739\n",
      " 0.95167904 0.9516791  0.95144836 0.95144821        nan 0.95144876\n",
      " 0.95144989 0.95123665 0.95123665        nan 0.95123527 0.9512368\n",
      " 0.95154713 0.95154707 0.95154694 0.95154658 0.95154826 0.95167724\n",
      " 0.95167712 0.95167733 0.95167901 0.95167858 0.9514483  0.95144818\n",
      "        nan 0.95144848 0.95145044 0.95123665 0.95123668        nan\n",
      " 0.9512353  0.95123683 0.9515471  0.9515471  0.95154694 0.95154652\n",
      " 0.95154768 0.95167727 0.95167712 0.95167733 0.95167904 0.95167886\n",
      " 0.95144833 0.95144821        nan 0.95144882 0.95144995 0.95123665\n",
      " 0.95123668        nan 0.95123518 0.95123655 0.9515471  0.9515471\n",
      " 0.95154694 0.95154661 0.95154905 0.95167727 0.95167712 0.95167733\n",
      " 0.95167901 0.95167901 0.95144833 0.95144821        nan 0.95144861\n",
      " 0.95144983 0.95123665 0.95123668        nan 0.9512353  0.95123655\n",
      " 0.9515471  0.9515471  0.95154694 0.95154655 0.95154783 0.95167727\n",
      " 0.95167712 0.95167733 0.95167907 0.95167895]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marta\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.95055131 0.95055125        nan 0.95055204 0.95055155 0.95052243\n",
      " 0.9505224         nan 0.95052319 0.95052307 0.9506091  0.95060864\n",
      " 0.95060363 0.95060922 0.95060833 0.95060014 0.95060008 0.95059996\n",
      " 0.95060027 0.95060097 0.95116754 0.95116757        nan 0.95116739\n",
      " 0.95116748 0.95103179 0.9510317         nan 0.9510324  0.95103158\n",
      " 0.9512569  0.95125693 0.95124079 0.95125699 0.95125681 0.95122646\n",
      " 0.95122646 0.95122606 0.95122622 0.95122683 0.95129192 0.95129213\n",
      "        nan 0.95129201 0.9512921  0.95103123 0.95103126        nan\n",
      " 0.95103163 0.95103157 0.95140786 0.95140786 0.95139069 0.95140786\n",
      " 0.95140765 0.95135481 0.95135481 0.9513549  0.95135447 0.95135509\n",
      " 0.95128684 0.95128687        nan 0.95128659 0.95128653 0.95097521\n",
      " 0.95097509        nan 0.95097509 0.95097417 0.95141452 0.95141452\n",
      " 0.9514125  0.95141547 0.95141477 0.95139438 0.95139432 0.9513949\n",
      " 0.95139499 0.95139463 0.95131684 0.95131688        nan 0.95131642\n",
      " 0.95131562 0.95102848 0.95102848        nan 0.95102808 0.95102802\n",
      " 0.95144707 0.95144707 0.95145263 0.95144707 0.95144664 0.95149862\n",
      " 0.95149862 0.95149862 0.95149881 0.95149874 0.95134527 0.95134527\n",
      "        nan 0.95134502 0.95134594 0.95110803 0.95110818        nan\n",
      " 0.9511076  0.95110711 0.95146192 0.95146198 0.95146721 0.95146238\n",
      " 0.95146164 0.95156567 0.95156561 0.95156567 0.95156445 0.95156439\n",
      " 0.95135068 0.95135074        nan 0.95135037 0.95135257 0.95115818\n",
      " 0.95115827        nan 0.9511586  0.95115708 0.95147155 0.95147152\n",
      " 0.95147546 0.9514713  0.95147014 0.95161002 0.95161014 0.95161008\n",
      " 0.95161035 0.95160996 0.95135294 0.95135288        nan 0.95135309\n",
      " 0.95135474 0.95117823 0.95117813        nan 0.95117688 0.95117749\n",
      " 0.95146693 0.95146672 0.95146813 0.95146702 0.95146886 0.95162863\n",
      " 0.95162869 0.95162866 0.95162875 0.95162814 0.95135404 0.95135422\n",
      "        nan 0.95135438 0.95135609 0.95118275 0.95118247        nan\n",
      " 0.95118287 0.95118272 0.95146632 0.95146629 0.95146635 0.95146657\n",
      " 0.95146608 0.95163328 0.95163315 0.95163312 0.9516334  0.95163285\n",
      " 0.95135291 0.95135297        nan 0.95135456 0.95135578 0.95118397\n",
      " 0.95118394        nan 0.951184   0.95118581 0.9514625  0.9514625\n",
      " 0.95146281 0.95146467 0.95146641 0.95163251 0.95163269 0.95163251\n",
      " 0.95163386 0.95163569 0.95135361 0.95135352        nan 0.95135175\n",
      " 0.95135697 0.95118465 0.95118458        nan 0.95118464 0.95118565\n",
      " 0.95146256 0.95146259 0.95146256 0.95146275 0.951468   0.95163288\n",
      " 0.95163269 0.9516326  0.95163379 0.95163658 0.95135374 0.95135374\n",
      "        nan 0.95135426 0.95135667 0.9511844  0.95118443        nan\n",
      " 0.9511851  0.95118666 0.95146275 0.95146272 0.95146293 0.95146269\n",
      " 0.95146767 0.95163269 0.95163273 0.95163251 0.95163413 0.95163651\n",
      " 0.95135334 0.95135334        nan 0.95135413 0.95135658 0.95118504\n",
      " 0.95118498        nan 0.95118507 0.95118639 0.95146308 0.95146305\n",
      " 0.95146293 0.95146284 0.95146904 0.95163306 0.951633   0.95163291\n",
      " 0.95163407 0.95163667 0.95135328 0.95135331        nan 0.95135401\n",
      " 0.95135639 0.95118538 0.95118544        nan 0.95118501 0.95118669\n",
      " 0.95146281 0.95146287 0.9514629  0.95146253 0.95146672 0.95163334\n",
      " 0.95163306 0.95163309 0.95163383 0.95163651 0.95135331 0.95135331\n",
      "        nan 0.95135309 0.9513571  0.95118541 0.95118544        nan\n",
      " 0.95118507 0.95118681 0.95146275 0.95146278 0.9514629  0.95146259\n",
      " 0.95146773 0.9516334  0.95163315 0.95163321 0.95163389 0.95163706\n",
      " 0.95135334 0.9513534         nan 0.95135288 0.95135664 0.95118541\n",
      " 0.9511855         nan 0.95118504 0.95118639 0.95146263 0.95146269\n",
      " 0.95146278 0.95146299 0.95146758 0.95163331 0.95163315 0.95163321\n",
      " 0.95163389 0.95163688 0.95135334 0.95135331        nan 0.95135319\n",
      " 0.95135627 0.95118541 0.95118553        nan 0.9511851  0.95118684\n",
      " 0.95146266 0.95146269 0.95146275 0.95146284 0.95146608 0.95163331\n",
      " 0.95163312 0.95163325 0.95163373 0.95163645 0.95135334 0.95135328\n",
      "        nan 0.95135447 0.95135667 0.95118547 0.95118553        nan\n",
      " 0.95118498 0.95118642 0.95146266 0.95146266 0.95146275 0.95146299\n",
      " 0.95146764 0.95163331 0.95163312 0.95163325 0.95163389 0.95163645\n",
      " 0.95135334 0.95135328        nan 0.95135444 0.95135679 0.95118547\n",
      " 0.95118553        nan 0.95118504 0.9511866  0.95146263 0.95146263\n",
      " 0.95146278 0.9514629  0.95146718 0.95163334 0.95163312 0.95163325\n",
      " 0.95163395 0.95163648 0.95135334 0.95135328        nan 0.95135447\n",
      " 0.95135667 0.95118547 0.95118553        nan 0.95118504 0.9511866\n",
      " 0.95146263 0.95146263 0.95146278 0.95146281 0.95146657 0.95163334\n",
      " 0.95163312 0.95163325 0.95163398 0.95163673]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marta\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.9497718  0.94977177        nan 0.94977217 0.94977147 0.94974637\n",
      " 0.94974634        nan 0.94974506 0.94974631 0.94983015 0.94982933\n",
      " 0.94982502 0.94982994 0.9498304  0.94982156 0.94982144 0.94982111\n",
      " 0.94982153 0.94982141 0.95049102 0.95049102        nan 0.9504909\n",
      " 0.95049075 0.95035635 0.95035635        nan 0.95035647 0.95035644\n",
      " 0.95058449 0.95058446 0.95056697 0.95058327 0.95058446 0.95054919\n",
      " 0.95054912 0.95054922 0.95054919 0.95054937 0.9507226  0.95072266\n",
      "        nan 0.95072238 0.95072244 0.95045601 0.95045601        nan\n",
      " 0.95045543 0.95045546 0.95084418 0.95084409 0.95082294 0.95084373\n",
      " 0.9508436  0.95078388 0.95078382 0.95078406 0.95078437 0.95078425\n",
      " 0.95078118 0.95078118        nan 0.95078002 0.9507813  0.9504578\n",
      " 0.95045774        nan 0.95045786 0.95045731 0.95091973 0.95091973\n",
      " 0.95091203 0.9509197  0.95091939 0.95088424 0.95088421 0.95088427\n",
      " 0.95088461 0.95088354 0.95083216 0.95083225        nan 0.95083216\n",
      " 0.9508321  0.95052626 0.95052626        nan 0.95052599 0.9505255\n",
      " 0.95096588 0.9509657  0.95097025 0.95096619 0.95096551 0.95099788\n",
      " 0.95099791 0.95099797 0.95099761 0.95099752 0.95088042 0.95088048\n",
      "        nan 0.95088115 0.95088112 0.95061077 0.95061074        nan\n",
      " 0.95061096 0.95061071 0.95100375 0.95100384 0.95100577 0.95100412\n",
      " 0.9510035  0.95108456 0.95108453 0.95108514 0.95108429 0.95108423\n",
      " 0.95090716 0.95090725        nan 0.95090783 0.95090796 0.95067636\n",
      " 0.95067639        nan 0.95067664 0.95067615 0.95101943 0.95101943\n",
      " 0.95102126 0.95101909 0.95101866 0.95113573 0.9511357  0.95113564\n",
      " 0.95113567 0.95113539 0.95091587 0.95091547        nan 0.95091499\n",
      " 0.95091697 0.95069828 0.95069825        nan 0.95069825 0.95069782\n",
      " 0.95101802 0.95101821 0.95101927 0.95101836 0.95101937 0.95115493\n",
      " 0.9511549  0.95115471 0.95115526 0.95115557 0.95091245 0.95091233\n",
      "        nan 0.95091321 0.95091664 0.95070464 0.95070467        nan\n",
      " 0.95070506 0.95070543 0.95101698 0.95101698 0.95101695 0.95101646\n",
      " 0.95101992 0.95116468 0.95116452 0.95116513 0.95116471 0.95116431\n",
      " 0.95091205 0.95091202        nan 0.95091281 0.95091728 0.95071167\n",
      " 0.95071167        nan 0.9507105  0.95070876 0.95101628 0.95101628\n",
      " 0.95101558 0.95101732 0.95101772 0.95116782 0.95116779 0.95116749\n",
      " 0.95116642 0.95116721 0.95091138 0.95091135        nan 0.95091144\n",
      " 0.9509167  0.95071206 0.95071209        nan 0.95071154 0.95071072\n",
      " 0.9510157  0.9510157  0.95101576 0.95101701 0.95101827 0.95116999\n",
      " 0.95116999 0.95116984 0.95116822 0.95116892 0.95091214 0.95091199\n",
      "        nan 0.95091294 0.95091709 0.95071212 0.95071219        nan\n",
      " 0.95071185 0.95071075 0.95101607 0.95101625 0.9510157  0.95101683\n",
      " 0.95101827 0.95116975 0.95116978 0.9511699  0.95116883 0.95116972\n",
      " 0.9509123  0.95091245        nan 0.95091245 0.9509166  0.95071179\n",
      " 0.95071185        nan 0.95071191 0.95071115 0.95101573 0.9510157\n",
      " 0.95101564 0.95101686 0.95101885 0.95117073 0.95117061 0.951171\n",
      " 0.95116944 0.95116981 0.95091242 0.95091236        nan 0.9509123\n",
      " 0.950917   0.95071194 0.95071191        nan 0.95071185 0.95071112\n",
      " 0.95101548 0.95101545 0.95101542 0.95101692 0.95101863 0.9511707\n",
      " 0.95117067 0.95117079 0.9511696  0.95117003 0.95091223 0.95091233\n",
      "        nan 0.95091217 0.95091663 0.95071197 0.95071194        nan\n",
      " 0.95071185 0.95071185 0.9510157  0.95101555 0.95101539 0.95101695\n",
      " 0.95101875 0.95117058 0.95117048 0.95117073 0.95116957 0.95116981\n",
      " 0.95091223 0.95091226        nan 0.95091278 0.95091602 0.95071194\n",
      " 0.95071194        nan 0.95071179 0.95071173 0.95101573 0.95101558\n",
      " 0.95101533 0.95101698 0.95101943 0.95117051 0.95117045 0.95117082\n",
      " 0.95116966 0.95117006 0.95091223 0.95091223        nan 0.95091254\n",
      " 0.95091691 0.95071194 0.95071194        nan 0.950712   0.95071167\n",
      " 0.9510157  0.95101561 0.95101536 0.95101698 0.95101866 0.95117048\n",
      " 0.95117042 0.95117079 0.95116966 0.95116978 0.95091223 0.9509122\n",
      "        nan 0.95091214 0.95091667 0.95071197 0.95071197        nan\n",
      " 0.95071191 0.95071191 0.95101573 0.95101558 0.95101536 0.95101701\n",
      " 0.95101799 0.95117048 0.95117039 0.95117076 0.95116975 0.95116984\n",
      " 0.95091223 0.9509122         nan 0.95091229 0.95091694 0.950712\n",
      " 0.95071197        nan 0.95071197 0.95071176 0.9510157  0.95101558\n",
      " 0.95101536 0.95101695 0.95101888 0.95117042 0.95117036 0.95117073\n",
      " 0.95116963 0.95116987 0.95091223 0.9509122         nan 0.95091214\n",
      " 0.950917   0.950712   0.95071197        nan 0.950712   0.9507117\n",
      " 0.9510157  0.95101558 0.95101536 0.95101692 0.95101875 0.95117042\n",
      " 0.95117036 0.95117073 0.95116951 0.95116987]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble: Mean: 0.960720758414304 Std: 0.002306849328364128\n"
     ]
    }
   ],
   "source": [
    "kfold = KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "cv_results = cross_val_score(voting_clf_, X_train, y_train, cv=kfold, scoring='roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcd8e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ensemble: Mean: {:>8} Std: {:>8}\".format(cv_results.mean(), cv_results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda940d6",
   "metadata": {},
   "source": [
    "**Output:** Ensemble: Mean: 0.960720758414304 Std: 0.002306849328364128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab5af9d",
   "metadata": {},
   "source": [
    "Finally, let's train and save the ensemble model in a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4182aa57",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n",
      "Fitting 5 folds for each of 80 candidates, totalling 400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marta\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n",
      "C:\\Users\\marta\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:918: UserWarning: One or more of the test scores are non-finite: [0.95063385 0.95063389        nan 0.95063395 0.95063383 0.95059184\n",
      " 0.95059176        nan 0.95059193 0.95059246 0.95069785 0.95069758\n",
      " 0.95069267 0.95069834 0.95069754 0.95068766 0.95068764 0.95068746\n",
      " 0.95068786 0.95068733 0.95114737 0.95114741        nan 0.95114799\n",
      " 0.95114745 0.95097755 0.95097766        nan 0.95097745 0.950978\n",
      " 0.95124417 0.95124415 0.95122451 0.95124444 0.9512439  0.95120071\n",
      " 0.95120075 0.95120071 0.95120028 0.95120049 0.95124578 0.95124609\n",
      "        nan 0.95124547 0.95124599 0.95095713 0.95095727        nan\n",
      " 0.95095719 0.9509568  0.95137069 0.95137088 0.95135527 0.95137067\n",
      " 0.95137082 0.951314   0.95131402 0.95131424 0.95131424 0.95131434\n",
      " 0.95127115 0.95127115        nan 0.95127162 0.95127133 0.95095143\n",
      " 0.95095135        nan 0.95095114 0.95095065 0.951405   0.95140498\n",
      " 0.95140268 0.95140496 0.95140473 0.9513902  0.95139024 0.95139028\n",
      " 0.95139061 0.95139026 0.95131346 0.95131363        nan 0.95131346\n",
      " 0.95131324 0.95102424 0.95102419        nan 0.95102409 0.95102409\n",
      " 0.95144725 0.95144723 0.95145169 0.95144743 0.95144707 0.95149664\n",
      " 0.95149686 0.95149713 0.95149646 0.95149599 0.9513577  0.95135772\n",
      "        nan 0.9513571  0.95135741 0.9511061  0.95110618        nan\n",
      " 0.95110552 0.95110606 0.95147356 0.95147352 0.95147657 0.95147316\n",
      " 0.95147258 0.95156927 0.95156927 0.95156931 0.95156962 0.95156997\n",
      " 0.95137355 0.95137349        nan 0.95137386 0.95137431 0.95115156\n",
      " 0.95115152        nan 0.95115193 0.95115148 0.95148643 0.9514866\n",
      " 0.95148856 0.95148586 0.9514865  0.95161717 0.95161715 0.95161719\n",
      " 0.95161662 0.95161582 0.95137885 0.95137885        nan 0.95137834\n",
      " 0.95137949 0.95117392 0.9511739         nan 0.95117366 0.95117392\n",
      " 0.95149214 0.95149214 0.95149347 0.95149167 0.95149087 0.95163182\n",
      " 0.95163176 0.95163162 0.95163182 0.95163213 0.95138121 0.9513811\n",
      "        nan 0.95138166 0.95138192 0.95117988 0.95117984        nan\n",
      " 0.95117941 0.95117955 0.95148991 0.95148995 0.95149071 0.95149147\n",
      " 0.95149202 0.95163704 0.95163671 0.95163687 0.95163738 0.95163745\n",
      " 0.9513811  0.95138119        nan 0.95138096 0.95138331 0.95118335\n",
      " 0.95118333        nan 0.95118237 0.95118159 0.95148979 0.95148981\n",
      " 0.95149001 0.95149067 0.95149145 0.95163751 0.95163753 0.95163757\n",
      " 0.95163882 0.95163929 0.95138119 0.95138129        nan 0.95138119\n",
      " 0.95138272 0.95118407 0.95118407        nan 0.95118358 0.95118364\n",
      " 0.95148967 0.95148965 0.95148954 0.95149044 0.95149157 0.95163798\n",
      " 0.95163798 0.9516381  0.9516391  0.95164025 0.95138137 0.95138143\n",
      "        nan 0.95138192 0.95138346 0.95118444 0.95118442        nan\n",
      " 0.95118411 0.95118382 0.95148877 0.95148877 0.95148899 0.9514896\n",
      " 0.95149167 0.95163816 0.95163814 0.95163835 0.95163959 0.95164045\n",
      " 0.95138149 0.95138139        nan 0.95138172 0.95138329 0.95118473\n",
      " 0.95118477        nan 0.95118436 0.95118374 0.95148891 0.95148889\n",
      " 0.95148883 0.95148981 0.9514919  0.95163831 0.95163837 0.95163826\n",
      " 0.95163982 0.95164021 0.95138145 0.95138098        nan 0.95138161\n",
      " 0.95138352 0.95118481 0.95118477        nan 0.95118425 0.95118383\n",
      " 0.95148881 0.95148883 0.95148897 0.95148995 0.95149171 0.95163812\n",
      " 0.9516381  0.95163826 0.9516399  0.95164043 0.95138151 0.95138112\n",
      "        nan 0.95138157 0.95138327 0.9511847  0.95118485        nan\n",
      " 0.95118432 0.95118389 0.95148887 0.95148885 0.95148897 0.95149001\n",
      " 0.95149165 0.95163806 0.95163804 0.95163816 0.95163976 0.95164053\n",
      " 0.95138155 0.95138118        nan 0.95138159 0.95138317 0.95118475\n",
      " 0.95118483        nan 0.95118438 0.95118376 0.95148885 0.95148887\n",
      " 0.95148899 0.95148989 0.95149181 0.95163794 0.95163792 0.95163816\n",
      " 0.9516397  0.95164053 0.95138153 0.95138116        nan 0.95138168\n",
      " 0.95138309 0.95118471 0.95118479        nan 0.95118436 0.95118376\n",
      " 0.95148881 0.95148883 0.95148895 0.95148989 0.95149171 0.9516379\n",
      " 0.95163792 0.95163814 0.95163978 0.95164058 0.95138155 0.95138116\n",
      "        nan 0.95138159 0.95138321 0.95118468 0.95118479        nan\n",
      " 0.95118432 0.95118391 0.95148881 0.95148885 0.95148895 0.95148999\n",
      " 0.95149181 0.95163792 0.95163794 0.95163816 0.95163974 0.95164051\n",
      " 0.95138159 0.95138116        nan 0.95138207 0.95138335 0.95118468\n",
      " 0.95118477        nan 0.95118442 0.95118401 0.95148881 0.95148885\n",
      " 0.95148895 0.95149001 0.95149192 0.95163792 0.95163792 0.95163816\n",
      " 0.95163976 0.95164049 0.95138159 0.95138118        nan 0.95138166\n",
      " 0.95138329 0.95118468 0.95118477        nan 0.95118444 0.9511838\n",
      " 0.95148883 0.95148885 0.95148897 0.95148989 0.95149163 0.95163792\n",
      " 0.95163792 0.95163816 0.95163965 0.95164051]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-36f271bebd1e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#Looking at the classification report\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": [
    "voting_clf_trained = voting_clf_.fit(X_train, y_train)\n",
    "\n",
    "# predict test set\n",
    "# y_pred = voting_clf_.predict(X_test)\n",
    "\n",
    "with open('models/voting_clf.pickle', 'wb') as file:\n",
    "    pickle.dump(voting_clf_trained, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dd093b",
   "metadata": {},
   "source": [
    "## 2.5. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7af6879b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/hypertuned_xgbc.pickle', 'rb') as file:\n",
    "    hypertuned_xgb = pickle.load(file)\n",
    "with open('models/hypertuned_rf.pickle', 'rb') as file:\n",
    "    hypertuned_rf = pickle.load(file)\n",
    "with open('models/hypertuned_logreg.pickle', 'rb') as file:\n",
    "    hypertuned_logreg = pickle.load(file)\n",
    "with open('models/voting_clf.pickle', 'rb') as file:\n",
    "    voting_clf = pickle.load(file)\n",
    "    \n",
    "train_results = {}\n",
    "train_results['XGBoost'] = hypertuned_xgb.best_score_\n",
    "train_results['Random Forest'] = hypertuned_rf.best_score_\n",
    "train_results['Logistic Regression'] = hypertuned_logreg.best_score_\n",
    "train_results['Ensemble'] = cv_results.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "447f24b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>AUC Before Tuning</th>\n",
       "      <th>AUC After Tuning</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.959365</td>\n",
       "      <td>0.961630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ensemble</td>\n",
       "      <td>--</td>\n",
       "      <td>0.960721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.956494</td>\n",
       "      <td>0.958965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.951391</td>\n",
       "      <td>0.951641</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Model Name AUC Before Tuning  AUC After Tuning\n",
       "0              XGBoost          0.959365          0.961630\n",
       "3             Ensemble                --          0.960721\n",
       "1        Random Forest          0.956494          0.958965\n",
       "2  Logistic Regression          0.951391          0.951641"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparative_table = []\n",
    "for name in model_names:\n",
    "    if name not in basic_results.keys():\n",
    "        basic_results[name] = \"--\"\n",
    "    comparative_table.append([name, basic_results[name], train_results[name]])\n",
    "\n",
    "models_comparative = pd.DataFrame(comparative_table, columns=['Model Name', 'AUC Before Tuning', 'AUC After Tuning'])\n",
    "models_comparative.sort_values(by=['AUC After Tuning'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc55f4",
   "metadata": {},
   "source": [
    "We see that, in the end, the ensemble model didn't give us better results (at least not in training) than all the rest. In this case the XGBoost Classifier is the one with the best training results. That means that the Random Forest and the Logistic Regression vote don't help the classification. \n",
    "\n",
    "In short: the best result in training we could find is obtained using the XGBoost model after tuning it's best parameters, which are:\n",
    "    \n",
    "    eta = 0.05\n",
    "    max_depth = 5\n",
    "    min_child_weight = 8\n",
    "\n",
    "(they can be printed using `hypertuned_xgb.best_params_`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4559ef51",
   "metadata": {},
   "source": [
    "# 3. Model testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3f7e59",
   "metadata": {},
   "source": [
    "Once we have trained all of our models (these were Logistic Regression, XGBoost, Random Forest and a voting ensemble of the three), let's make a prediction with our test data to really see which one will be better in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca9c577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction vectors for both train and test\n",
    "y_pred_log_reg_train = hypertuned_logreg.predict(X_train)\n",
    "y_pred_log_reg_test = hypertuned_logreg.predict(X_test)\n",
    "\n",
    "y_pred_xgboost_train = hypertuned_xgb.predict(X_train)\n",
    "y_pred_xgboost_test = hypertuned_xgb.predict(X_test)\n",
    "\n",
    "y_pred_rf_train = hypertuned_rf.predict(X_train)\n",
    "y_pred_rf_test = hypertuned_rf.predict(X_test)\n",
    "\n",
    "y_pred_ensemble_train = voting_clf_.predict(X_train)\n",
    "y_pred_ensemble_test = voting_clf_.predict(X_test)\n",
    "\n",
    "train_predictions = {'Logistic Regression' : y_pred_log_reg_train,\n",
    "                     'XGBoost' : y_pred_xgboost_train,\n",
    "                     'Random Forest' : y_pred_rf_train,\n",
    "                     'Ensemble' : y_pred_ensemble_train}\n",
    "\n",
    "test_predictions = {'Logistic Regression' : y_pred_log_reg_test,\n",
    "                     'XGBoost' : y_pred_xgboost_test,\n",
    "                     'Random Forest' : y_pred_rf_test,\n",
    "                     'Ensemble' : y_pred_ensemble_test}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bdacf5",
   "metadata": {},
   "source": [
    "Let's first try to use the `classification_report` function from `sklearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a08b75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0620910f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classification report for {name} is:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93      4658\n",
      "           1       0.89      0.93      0.91      3392\n",
      "\n",
      "    accuracy                           0.92      8050\n",
      "   macro avg       0.92      0.92      0.92      8050\n",
      "weighted avg       0.92      0.92      0.92      8050\n",
      "\n",
      "The classification report for {name} is:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93      4634\n",
      "           1       0.89      0.93      0.91      3416\n",
      "\n",
      "    accuracy                           0.92      8050\n",
      "   macro avg       0.92      0.92      0.92      8050\n",
      "weighted avg       0.92      0.92      0.92      8050\n",
      "\n",
      "The classification report for {name} is:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93      4605\n",
      "           1       0.89      0.93      0.91      3445\n",
      "\n",
      "    accuracy                           0.92      8050\n",
      "   macro avg       0.92      0.92      0.92      8050\n",
      "weighted avg       0.92      0.92      0.92      8050\n",
      "\n",
      "The classification report for {name} is:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.91      0.93      4638\n",
      "           1       0.89      0.93      0.91      3412\n",
      "\n",
      "    accuracy                           0.92      8050\n",
      "   macro avg       0.92      0.92      0.92      8050\n",
      "weighted avg       0.92      0.92      0.92      8050\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for model in model_names:\n",
    "    print(f\"The classification report for {name} is:\")\n",
    "    print(classification_report(test_predictions[model], y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8211cc",
   "metadata": {},
   "source": [
    "Strikingly, we get the exact same results for all of them. Let's see if we can see this metrics with a little bit more precision. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c02ea6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy using XGBoost is 0.9222360248447204.\n",
      "The accuracy using Random Forest is 0.9214906832298136.\n",
      "The accuracy using Logistic Regression is 0.9198757763975155.\n",
      "The accuracy using Ensemble is 0.9219875776397516.\n"
     ]
    }
   ],
   "source": [
    "def accuracy(pred, test):\n",
    "    return sum([1 if pred[i] == test[idx] else 0 for i,idx in enumerate(test.index)])/len(test)\n",
    "\n",
    "for model in model_names:\n",
    "    print(f\"The accuracy using {model} is {accuracy(test_predictions[model], y_test)}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c826be21",
   "metadata": {},
   "source": [
    "Meaning the results are really similar, regardless of the model, when we use the prediction metric. The best one is still the XGBoost classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6b1b4e",
   "metadata": {},
   "source": [
    "To wrap it all up, let's save the prediction results in a pickle file as well, so that we can just acces them, if ever we need to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "568cb827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving train and test predictions\n",
    "with open('predictions/train_predictions.pickle', 'wb') as file:\n",
    "    pickle.dump(train_predictions, file)\n",
    "\n",
    "with open('predictions/test_predictions.pickle', 'wb') as file:\n",
    "    pickle.dump(test_predictions, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955c04b2",
   "metadata": {},
   "source": [
    "# 4. Conclusions and Future Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58305eb3",
   "metadata": {},
   "source": [
    "This dataset gave really good results from the beggining, even before we tuned or models or used the ensemble model. Even then, it was still a really workable and flexible dataset that offered room for new variables to work with.\n",
    "\n",
    "I order to improve this classification, there are a couple things we could do. In terms of approach, we could try to create a model that would not only classify between dwarf and giants, but also specify further and determine the type of dwarf or the type of giant that the star is actually classified as. \n",
    "\n",
    "In terms of modeling, we could have tried our models with a different preprocessed data (for example with min-max normalization instead of 0 to 1). It would also be a good idea to find the best parameters for the `N-Neighbours Clasifier` and add it to the voting ensemble, although the results would probably be the same or insignicantly different. Lastly, we could have tried with a different type of ensemble; we tried with a *soft* voting system, meaning  that the final decision was made using the average score for all models. We could have tried a *hard* voting system to potencially get different results.\n",
    "\n",
    "As well, we could have tried more models, although that improvement offers a list of possibilities that would never end. \n",
    "\n",
    "In the end, this dataset was easy to work with and has been really helpful to learn more about classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ce59d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
